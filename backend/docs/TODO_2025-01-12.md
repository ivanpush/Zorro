# ZORRO TODO - 2025-01-12

*Created: 2025-01-11 (for tomorrow)*

---

## Summary of Today's Work (2025-01-11)

### Completed
- Upgraded all agents from `claude-3-5-haiku-20241022` to `claude-haiku-4-5-20251001`
- Added `temperature=0` globally for deterministic outputs
- Redesigned dev banner: collapsible card in bottom-right with per-agent breakdown
- Backend now sends `agent_metrics` in `ReviewCompletedEvent`
- Fixed Suggestion box styling to match Suggested Rewrite (amber vs teal)
- Enforced suggestions in Rigor: every finding now gets either a rewrite OR suggestion
  - Updated prompts to clarify the distinction
  - Added fallback in `rewriter.py` if LLM skips an issue

---

## Priority 1: Conflicting Paragraph Edits

**Problem**: When multiple findings target the same paragraph:
- Finding A suggests rewriting the entire paragraph
- Finding B suggests editing just one sentence within it
- When user accepts Finding A's rewrite, Finding B's edit "disappears" because its anchor no longer exists in the new text

**Possible Approaches**:

### Option A: Anchor Invalidation + Warning
- When a rewrite is applied, scan other findings for overlapping anchors
- Mark those findings as "invalidated" (their anchor text no longer exists)
- Show user a warning: "Accepting this rewrite will invalidate 2 other suggestions"
- Auto-dismiss or flag invalidated findings

### Option B: Hierarchical Edit Grouping
- Group findings by paragraph
- Show them as a tree: paragraph-level edits at top, sentence-level nested
- User picks ONE approach per paragraph (either the big rewrite or individual sentence fixes)
- Mutually exclusive selection within a paragraph

### Option C: Smart Merge (Complex)
- When accepting a paragraph rewrite, try to "replay" sentence-level edits on the new text
- Use fuzzy matching to find where the sentence-level anchor moved to
- High complexity, might produce weird results

### Recommended: Option A
- Simplest to implement
- Transparent to user (they see the warning)
- No magic/confusion about what happened

**Implementation Notes**:
1. In `ReviewScreen`, when `handleAcceptRewrite` is called:
   - Get the paragraph ID of the accepted rewrite
   - Find all other findings with anchors in that paragraph
   - Check if their `quotedText` still exists in the new text
   - If not, auto-dismiss them OR show warning first
2. Add visual indicator for "at risk" findings when hovering over a rewrite

---

## Priority 2: Enforce Suggestions Everywhere

**Status**: Partially done for Rigor

**Remaining**:
- [ ] Verify Clarity agent outputs have suggestions (or add if missing)
- [ ] Verify Adversary agent outputs have suggestions
- [ ] Add similar prompt enforcement to Clarity if needed
- [ ] Add similar prompt enforcement to Adversary if needed
- [ ] Test full pipeline end-to-end

---

## Priority 3: Wire Everything Up

- [ ] Full integration test: upload doc -> process -> review -> all findings have rewrite/suggestion
- [ ] Verify SSE events are flowing correctly with new metrics
- [ ] Test dev banner shows accurate per-agent breakdown
- [ ] Ensure findings panel displays both rewrites (teal) and suggestions (amber) correctly

---

## Priority 4: Agent Polish Order

1. **Clarity** - Should be mostly working, verify suggestions
2. **Rigor** - Just updated, needs testing
3. **Adversary** - Need to verify it produces actionable output
4. **Domain** - Last (currently disabled in settings)

---

## Priority 5: Optimize Prompts

Once agents are wired up and working:
- [ ] Audit each agent's prompt for clarity and efficiency
- [ ] Reduce token usage where possible
- [ ] Improve output quality/consistency
- [ ] A/B test prompt variations

---

## Priority 6: Domain Agent + Clickable References

**Problem**: Domain agent findings will include references (e.g., "[1] Smith et al. 2023").
These show up as plain text numbers in issue cards - need to be clickable links.

**Design Considerations**:
1. Backend sends references with URLs in `EvidencePack`
2. Frontend needs to:
   - Parse reference markers like "[1]", "[2]" in finding descriptions
   - Map them to the actual reference data (title, URL, authors)
   - Render as clickable links that open in new tab
   - Hover tooltip showing full citation

**Implementation Options**:

### Option A: Structured References in Finding
```typescript
interface Finding {
  // ... existing fields
  references?: Array<{
    marker: string;      // "[1]"
    title: string;
    url: string;
    authors?: string;
    year?: string;
  }>;
}
```
- Frontend renders `[1]` as `<a href={url}>[1]</a>` with tooltip
- Clean separation of data

### Option B: Markdown-style Links
- Backend outputs: `[1](https://example.com)` in description
- Frontend parses markdown links
- Simpler but less structured

### Recommended: Option A
- More control over rendering
- Can show rich tooltips
- Consistent with our typed approach

**UI Mockup**:
```
"This claim contradicts findings from [1] and [2] which show..."
                                      ^       ^
                                   clickable, hover shows:
                                   "Smith et al. (2023) - Nature"
```

---

## Notes

- Domain agent is disabled in `settings.py` (`enable_domain=False`)
- Max concurrent agents increased to 10
- All agents now use Haiku 4.5 with temperature=0
